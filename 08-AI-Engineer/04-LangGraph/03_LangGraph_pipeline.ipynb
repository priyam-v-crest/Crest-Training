{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "fG5W02Rdd9Ki",
        "outputId": "8cf72a19-6fa1-49a5-da43-7893b5bf8dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (75.2.0)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools, pip\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pip-25.3 setuptools-80.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "f2bc32cd4fc74e21b92525ab62056887"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: qdrant-client 1.15.1 does not provide the extra 'grpc'\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.38.0 which is incompatible.\n",
            "google-adk 1.17.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.38.0 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.38.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement langsmith-client (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langsmith-client\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# ===========================================\n",
        "# âœ… LANGGRAPH + LANGCHAIN + PINECONE ENV SETUP (Nov 2025)\n",
        "# ===========================================\n",
        "!pip install -U pip setuptools wheel\n",
        "\n",
        "# ---- Core GenAI Stack ----\n",
        "!pip install -qU \\\n",
        "  langgraph langchain langchain-core langchain-community \\\n",
        "  langchainhub langchain-huggingface \\\n",
        "  tiktoken huggingface-hub\n",
        "\n",
        "# ---- Vector DBs / Embeddings ----\n",
        "!pip install -qU \\\n",
        "  pinecone \\\n",
        "  chromadb qdrant-client[grpc] sentence-transformers\n",
        "\n",
        "# ---- Optional Observability / Utils ----\n",
        "!pip install -qU langsmith-client langfuse rich"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, importlib\n",
        "from importlib.metadata import version, PackageNotFoundError\n",
        "from rich.console import Console\n",
        "\n",
        "console = Console()\n",
        "\n",
        "console.rule(\"[bold cyan]LangGraph Stack Diagnostic (Nov 2025)\")\n",
        "pkgs = [\n",
        "    \"langgraph\",\n",
        "    \"langchain\",\n",
        "    \"langchain-core\",\n",
        "    \"langchain-community\",\n",
        "    \"langchainhub\",\n",
        "    \"pinecone\",\n",
        "    \"huggingface_hub\",\n",
        "    \"tiktoken\",\n",
        "    \"sentence_transformers\",\n",
        "    \"chromadb\",\n",
        "    \"qdrant_client\",\n",
        "]\n",
        "\n",
        "for pkg in pkgs:\n",
        "    try:\n",
        "        console.print(f\"âœ… {pkg} [green]{version(pkg)}[/green]\")\n",
        "    except PackageNotFoundError:\n",
        "        console.print(f\"âŒ {pkg} not installed\")\n",
        "\n",
        "# ---- GPU Check ----\n",
        "gpu_available = torch.cuda.is_available()\n",
        "console.rule(\"[bold yellow]System\")\n",
        "console.print(f\"GPU Available: [bold green]{gpu_available}[/bold green]\" if gpu_available else \"[bold red]GPU not detected[/bold red]\")\n",
        "\n",
        "# ---- LangGraph Core Check ----\n",
        "try:\n",
        "    from langgraph.graph import Graph, START, END, node\n",
        "\n",
        "    @node\n",
        "    def greet(name: str) -> str:\n",
        "        return f\"Hello, {name} ğŸ‘‹\"\n",
        "\n",
        "    with Graph() as g:\n",
        "        greet(START) >> END\n",
        "\n",
        "    app = g.compile()\n",
        "    result = app.invoke(\"LangGraph\")\n",
        "    console.print(f\"[bold green]LangGraph operational test passed:[/bold green] {result}\")\n",
        "except Exception as e:\n",
        "    console.print(f\"[bold red]LangGraph test failed:[/bold red] {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "9obM71QQeBbn",
        "outputId": "771747b1-f2f1-4f91-83af-c28267ad1bc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0m\u001b[1;36mLangGraph Stack Diagnostic \u001b[0m\u001b[1;36m(\u001b[0m\u001b[1;36mNov \u001b[0m\u001b[1;36m2025\u001b[0m\u001b[1;36m)\u001b[0m\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">LangGraph Stack Diagnostic (Nov </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">)</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… langgraph \u001b[1;32m1.0\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… langgraph <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.0</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… langchain \u001b[1;32m1.0\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m3\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… langchain <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.0</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… langchain-core \u001b[1;32m1.0\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m3\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… langchain-core <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.0</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">3</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… langchain-community \u001b[1;32m0.4\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m1\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… langchain-community <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.4</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… langchainhub \u001b[1;32m0.1\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m21\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… langchainhub <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.1</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">21</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… pinecone \u001b[1;32m7.3\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m0\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… pinecone <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">7.3</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… huggingface_hub \u001b[1;32m0.36\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m0\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… huggingface_hub <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.36</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… tiktoken \u001b[1;32m0.12\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m0\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… tiktoken <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.12</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… sentence_transformers \u001b[1;32m5.1\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m2\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… sentence_transformers <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">5.1</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">2</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… chromadb \u001b[1;32m1.3\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m4\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… chromadb <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.3</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">4</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "âœ… qdrant_client \u001b[1;32m1.15\u001b[0m\u001b[32m.\u001b[0m\u001b[1;32m1\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ… qdrant_client <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1.15</span><span style=\"color: #008000; text-decoration-color: #008000\">.</span><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">1</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[92mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ \u001b[0m\u001b[1;33mSystem\u001b[0m\u001b[92m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">System</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mGPU not detected\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">GPU not detected</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;31mLangGraph test failed:\u001b[0m cannot import name \u001b[32m'Graph'\u001b[0m from \u001b[32m'langgraph.graph'\u001b[0m \n",
              "\u001b[1m(\u001b[0m\u001b[35m/usr/local/lib/python3.12/dist-packages/langgraph/graph/\u001b[0m\u001b[95m__init__.py\u001b[0m\u001b[1m)\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">LangGraph test failed:</span> cannot import name <span style=\"color: #008000; text-decoration-color: #008000\">'Graph'</span> from <span style=\"color: #008000; text-decoration-color: #008000\">'langgraph.graph'</span> \n",
              "<span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080\">/usr/local/lib/python3.12/dist-packages/langgraph/graph/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">__init__.py</span><span style=\"font-weight: bold\">)</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# âœ… LANGGRAPH FUNDAMENTALS â€” Fixed for v1.2+\n",
        "# ===========================================\n",
        "\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# 1ï¸âƒ£ Define your state schema\n",
        "class MyState(TypedDict, total=False):\n",
        "    name: str\n",
        "    greeting: str\n",
        "    compliment: str\n",
        "    farewell: str\n",
        "\n",
        "# 2ï¸âƒ£ Define simple workflow nodes\n",
        "def greet(state: MyState) -> MyState:\n",
        "    name = state.get(\"name\", \"LangGraph User\")\n",
        "    state[\"greeting\"] = f\"Hello, {name} ğŸ‘‹\"\n",
        "    return state\n",
        "\n",
        "def compliment(state: MyState) -> MyState:\n",
        "    state[\"compliment\"] = f\"{state['greeting']} â€” You're awesome! ğŸš€\"\n",
        "    return state\n",
        "\n",
        "def farewell(state: MyState) -> MyState:\n",
        "    state[\"farewell\"] = f\"{state['compliment']}\\nGoodbye from LangGraph ğŸŒ™\"\n",
        "    return state\n",
        "\n",
        "# 3ï¸âƒ£ Build the graph\n",
        "graph = StateGraph(MyState)\n",
        "\n",
        "graph.add_node(\"greet\", greet)\n",
        "graph.add_node(\"compliment\", compliment)\n",
        "graph.add_node(\"farewell\", farewell)\n",
        "\n",
        "graph.set_entry_point(\"greet\")\n",
        "graph.add_edge(\"greet\", \"compliment\")\n",
        "graph.add_edge(\"compliment\", \"farewell\")\n",
        "graph.set_finish_point(\"farewell\")\n",
        "\n",
        "# 4ï¸âƒ£ Compile & Run\n",
        "app = graph.compile()\n",
        "\n",
        "result = app.invoke({\"name\": \"Priyam\"})\n",
        "print(\"ğŸ§© Graph Execution Output:\\n\")\n",
        "print(result[\"farewell\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlCSNLgEeDfM",
        "outputId": "227118a6-78a8-414a-cb56-4473ac9296e5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§© Graph Execution Output:\n",
            "\n",
            "Hello, Priyam ğŸ‘‹ â€” You're awesome! ğŸš€\n",
            "Goodbye from LangGraph ğŸŒ™\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# âœ… CELL 4 â€” LangGraph Environment Diagnostic + Graph Introspection (v1.2+)\n",
        "# Compatible with LangGraph 1.2.xâ€“1.3.x (Nov 2025)\n",
        "# ===========================================\n",
        "\n",
        "import importlib.metadata\n",
        "import sys, platform, psutil\n",
        "from typing import TypedDict\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# ---- 1ï¸âƒ£ ENVIRONMENT REPORT ----\n",
        "print(\"ğŸ” LANGGRAPH ENVIRONMENT DIAGNOSTIC\\n\" + \"=\"*45)\n",
        "\n",
        "def version_of(pkg):\n",
        "    try:\n",
        "        return importlib.metadata.version(pkg)\n",
        "    except importlib.metadata.PackageNotFoundError:\n",
        "        return \"âŒ Not Installed\"\n",
        "\n",
        "pkgs = [\n",
        "    \"langgraph\", \"langchain\", \"langchain-core\",\n",
        "    \"langchain-community\", \"huggingface_hub\",\n",
        "    \"tiktoken\", \"pinecone\"\n",
        "]\n",
        "\n",
        "for p in pkgs:\n",
        "    print(f\"{p:22}: {version_of(p)}\")\n",
        "\n",
        "print(\"\\nğŸ§  Python:\", sys.version.split()[0])\n",
        "print(\"ğŸ’» Platform:\", platform.platform())\n",
        "print(f\"ğŸ§© CPU cores: {psutil.cpu_count(logical=True)}\")\n",
        "print(f\"ğŸ§  Memory: {round(psutil.virtual_memory().total / 1e9, 2)} GB\")\n",
        "print(\"=\"*45, \"\\n\")\n",
        "\n",
        "# ---- 2ï¸âƒ£ BUILD TEST GRAPH ----\n",
        "class MyState(TypedDict, total=False):\n",
        "    name: str\n",
        "    greeting: str\n",
        "    compliment: str\n",
        "    farewell: str\n",
        "\n",
        "def greet(state: MyState) -> MyState:\n",
        "    name = state.get(\"name\", \"LangGraph User\")\n",
        "    state[\"greeting\"] = f\"Hello, {name} ğŸ‘‹\"\n",
        "    return state\n",
        "\n",
        "def compliment(state: MyState) -> MyState:\n",
        "    state[\"compliment\"] = f\"{state['greeting']} â€” You're awesome! ğŸš€\"\n",
        "    return state\n",
        "\n",
        "def farewell(state: MyState) -> MyState:\n",
        "    state[\"farewell\"] = f\"{state['compliment']}\\nGoodbye from LangGraph ğŸŒ™\"\n",
        "    return state\n",
        "\n",
        "graph = StateGraph(MyState)\n",
        "graph.add_node(\"greet\", greet)\n",
        "graph.add_node(\"compliment\", compliment)\n",
        "graph.add_node(\"farewell\", farewell)\n",
        "\n",
        "graph.set_entry_point(\"greet\")\n",
        "graph.add_edge(\"greet\", \"compliment\")\n",
        "graph.add_edge(\"compliment\", \"farewell\")\n",
        "graph.set_finish_point(\"farewell\")\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "# ---- 3ï¸âƒ£ EXECUTION TEST ----\n",
        "print(\"ğŸš€ Testing graph execution...\\n\")\n",
        "result = app.invoke({\"name\": \"Priyam\"})\n",
        "print(\"âœ… Graph ran successfully!\\n\")\n",
        "print(\"Output Preview:\\n----------------\")\n",
        "print(result[\"farewell\"])\n",
        "print(\"\\n\")\n",
        "\n",
        "# ---- 4ï¸âƒ£ GRAPH STRUCTURE INSPECTION ----\n",
        "print(\"ğŸ§© GRAPH STRUCTURE\\n\" + \"=\"*45)\n",
        "print(\"Nodes:\")\n",
        "for n in graph.nodes:\n",
        "    print(\" -\", n)\n",
        "\n",
        "print(\"\\nEdges:\")\n",
        "for e in graph.edges:\n",
        "    print(\" -\", e[0], \"â†’\", e[1])\n",
        "\n",
        "# âœ… Access entry/finish points safely from compiled app.config\n",
        "cfg = getattr(app, \"config\", {})\n",
        "entry = getattr(cfg, \"entry_point\", None)\n",
        "finish = getattr(cfg, \"finish_point\", None)\n",
        "print(\"\\nEntry Point:\", entry or \"âš ï¸ Unavailable in this build\")\n",
        "print(\"Finish Point:\", finish or \"âš ï¸ Unavailable in this build\")\n",
        "print(\"=\"*45)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUcPw1eNet3H",
        "outputId": "52cb97be-d43f-45c6-ae13-6923a475c4da"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” LANGGRAPH ENVIRONMENT DIAGNOSTIC\n",
            "=============================================\n",
            "langgraph             : 1.0.2\n",
            "langchain             : 1.0.3\n",
            "langchain-core        : 1.0.3\n",
            "langchain-community   : 0.4.1\n",
            "huggingface_hub       : 0.36.0\n",
            "tiktoken              : 0.12.0\n",
            "pinecone              : 7.3.0\n",
            "\n",
            "ğŸ§  Python: 3.12.12\n",
            "ğŸ’» Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "ğŸ§© CPU cores: 2\n",
            "ğŸ§  Memory: 13.61 GB\n",
            "============================================= \n",
            "\n",
            "ğŸš€ Testing graph execution...\n",
            "\n",
            "âœ… Graph ran successfully!\n",
            "\n",
            "Output Preview:\n",
            "----------------\n",
            "Hello, Priyam ğŸ‘‹ â€” You're awesome! ğŸš€\n",
            "Goodbye from LangGraph ğŸŒ™\n",
            "\n",
            "\n",
            "ğŸ§© GRAPH STRUCTURE\n",
            "=============================================\n",
            "Nodes:\n",
            " - greet\n",
            " - compliment\n",
            " - farewell\n",
            "\n",
            "Edges:\n",
            " - __start__ â†’ greet\n",
            " - farewell â†’ __end__\n",
            " - compliment â†’ farewell\n",
            " - greet â†’ compliment\n",
            "\n",
            "Entry Point: âš ï¸ Unavailable in this build\n",
            "Finish Point: âš ï¸ Unavailable in this build\n",
            "=============================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# âœ… CELL 5 â€” Conditional Logic & Dynamic Edge Traversal\n",
        "# Works for LangGraph 1.2.xâ€“1.3.x (Nov 2025)\n",
        "# ===========================================\n",
        "\n",
        "from typing import TypedDict, Literal\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# ---- 1ï¸âƒ£ DEFINE STATE MODEL ----\n",
        "class MoodState(TypedDict, total=False):\n",
        "    name: str\n",
        "    mood: Literal[\"happy\", \"sad\", \"neutral\"]\n",
        "    message: str\n",
        "    final: str\n",
        "\n",
        "\n",
        "# ---- 2ï¸âƒ£ DEFINE NODE FUNCTIONS ----\n",
        "def ask_mood(state: MoodState) -> MoodState:\n",
        "    \"\"\"Simulate reading user mood from input/state.\"\"\"\n",
        "    name = state.get(\"name\", \"Friend\")\n",
        "    mood = state.get(\"mood\", \"neutral\")\n",
        "    state[\"message\"] = f\"Hi {name}! I see you're feeling {mood} today.\"\n",
        "    return state\n",
        "\n",
        "\n",
        "def cheer_up(state: MoodState) -> MoodState:\n",
        "    \"\"\"Triggered if mood is sad.\"\"\"\n",
        "    state[\"final\"] = f\"{state['message']} ğŸŒ» Remember, every cloud has a silver lining!\"\n",
        "    return state\n",
        "\n",
        "\n",
        "def celebrate(state: MoodState) -> MoodState:\n",
        "    \"\"\"Triggered if mood is happy.\"\"\"\n",
        "    state[\"final\"] = f\"{state['message']} ğŸ‰ That's awesome! Keep the good vibes going!\"\n",
        "    return state\n",
        "\n",
        "\n",
        "def neutral_chat(state: MoodState) -> MoodState:\n",
        "    \"\"\"Triggered if mood is neutral.\"\"\"\n",
        "    state[\"final\"] = f\"{state['message']} ğŸ¤” Let's make today interesting!\"\n",
        "    return state\n",
        "\n",
        "\n",
        "# ---- 3ï¸âƒ£ DEFINE CONDITIONAL EDGE LOGIC ----\n",
        "def mood_router(state: MoodState):\n",
        "    \"\"\"Determine next node dynamically based on mood.\"\"\"\n",
        "    mood = state.get(\"mood\", \"neutral\")\n",
        "    if mood == \"happy\":\n",
        "        return \"celebrate\"\n",
        "    elif mood == \"sad\":\n",
        "        return \"cheer_up\"\n",
        "    else:\n",
        "        return \"neutral_chat\"\n",
        "\n",
        "\n",
        "# ---- 4ï¸âƒ£ BUILD GRAPH ----\n",
        "graph = StateGraph(MoodState)\n",
        "\n",
        "graph.add_node(\"ask_mood\", ask_mood)\n",
        "graph.add_node(\"cheer_up\", cheer_up)\n",
        "graph.add_node(\"celebrate\", celebrate)\n",
        "graph.add_node(\"neutral_chat\", neutral_chat)\n",
        "\n",
        "graph.set_entry_point(\"ask_mood\")\n",
        "graph.add_conditional_edges(\"ask_mood\", mood_router)\n",
        "graph.add_edge(\"cheer_up\", END)\n",
        "graph.add_edge(\"celebrate\", END)\n",
        "graph.add_edge(\"neutral_chat\", END)\n",
        "\n",
        "app = graph.compile()\n",
        "\n",
        "\n",
        "# ---- 5ï¸âƒ£ EXECUTION TESTS ----\n",
        "print(\"ğŸš€ Conditional Graph Execution Tests\\n\" + \"=\"*45)\n",
        "\n",
        "scenarios = [\n",
        "    {\"name\": \"Priyam\", \"mood\": \"happy\"},\n",
        "    {\"name\": \"Alex\", \"mood\": \"sad\"},\n",
        "    {\"name\": \"Jordan\", \"mood\": \"neutral\"},\n",
        "    {\"name\": \"Taylor\"},  # Default fallback (neutral)\n",
        "]\n",
        "\n",
        "for case in scenarios:\n",
        "    print(f\"\\nğŸ§© Input: {case}\")\n",
        "    output = app.invoke(case)\n",
        "    print(\"âœ… Output:\", output[\"final\"])\n",
        "    print(\"-\" * 45)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGw7IOpSfth0",
        "outputId": "ca4c3129-f4fb-4110-c70c-a7f3c3a19fcb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Conditional Graph Execution Tests\n",
            "=============================================\n",
            "\n",
            "ğŸ§© Input: {'name': 'Priyam', 'mood': 'happy'}\n",
            "âœ… Output: Hi Priyam! I see you're feeling happy today. ğŸ‰ That's awesome! Keep the good vibes going!\n",
            "---------------------------------------------\n",
            "\n",
            "ğŸ§© Input: {'name': 'Alex', 'mood': 'sad'}\n",
            "âœ… Output: Hi Alex! I see you're feeling sad today. ğŸŒ» Remember, every cloud has a silver lining!\n",
            "---------------------------------------------\n",
            "\n",
            "ğŸ§© Input: {'name': 'Jordan', 'mood': 'neutral'}\n",
            "âœ… Output: Hi Jordan! I see you're feeling neutral today. ğŸ¤” Let's make today interesting!\n",
            "---------------------------------------------\n",
            "\n",
            "ğŸ§© Input: {'name': 'Taylor'}\n",
            "âœ… Output: Hi Taylor! I see you're feeling neutral today. ğŸ¤” Let's make today interesting!\n",
            "---------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-45b8g6yjg10",
        "outputId": "3dae479e-fcb7-45ce-a093-d28a1a58c84d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-3.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.0.3)\n",
            "Collecting google-ai-generativelanguage<1.0.0,>=0.7.0 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.10)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.28.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.32.5)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.4.38)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-3.0.1-py3-none-any.whl (58 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "\u001b[2K  Attempting uninstall: google-ai-generativelanguage\n",
            "\u001b[2K    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "\u001b[2K    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "\u001b[2K      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [langchain-google-genai]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.9.0 langchain-google-genai-3.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "79d38f64c5ca436a9b92cef24aeb7ad9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "# âœ… GEMINI AUTH FIX for Colab (Nov 2025)\n",
        "# ==============================================\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# ğŸ”‘ Replace with your actual Gemini API key from https://aistudio.google.com/app/apikey\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# Verify key presence\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "    raise ValueError(\"âŒ Missing Gemini API key! Please set GOOGLE_API_KEY first.\")\n",
        "\n",
        "print(\"âœ… Gemini API key configured successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAdy_r-rkhmt",
        "outputId": "d68ba889-f9bd-4fc9-a9e4-f57a302aff2e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Gemini API key configured successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# âœ… CELL 7 â€” Gemini Conversational LangGraph (Final Stable)\n",
        "# - Handles MAX_TOKENS cutoffs\n",
        "# - Robust LLM extraction\n",
        "# - Returns proper mood + reply each time\n",
        "# ===========================================\n",
        "\n",
        "from typing import TypedDict, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain.chat_models import init_chat_model\n",
        "import time\n",
        "\n",
        "# ---- 1ï¸âƒ£ State model ----\n",
        "class ChatState(TypedDict, total=False):\n",
        "    messages: List[dict]\n",
        "    mood: str\n",
        "    reply: str\n",
        "\n",
        "# ---- 2ï¸âƒ£ LLM setup ----\n",
        "def make_llm(max_tokens=512):\n",
        "    return init_chat_model(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        model_provider=\"google_genai\",\n",
        "        temperature=0.6,\n",
        "        max_output_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "llm = make_llm()\n",
        "\n",
        "# ---- 3ï¸âƒ£ Universal extractor ----\n",
        "def extract_text(result):\n",
        "    if isinstance(result, str):\n",
        "        return result.strip()\n",
        "    if hasattr(result, \"content\") and result.content:\n",
        "        return result.content.strip()\n",
        "    if hasattr(result, \"text\") and result.text:\n",
        "        return result.text.strip()\n",
        "    if hasattr(result, \"candidates\"):\n",
        "        try:\n",
        "            c = result.candidates[0]\n",
        "            if hasattr(c, \"content\") and getattr(c.content, \"parts\", None):\n",
        "                parts = c.content.parts\n",
        "                if parts and hasattr(parts[0], \"text\"):\n",
        "                    return parts[0].text.strip()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if isinstance(result, dict):\n",
        "        if \"text\" in result:\n",
        "            return result[\"text\"].strip()\n",
        "    return \"\"\n",
        "\n",
        "# ---- 4ï¸âƒ£ Safe LLM caller with retry ----\n",
        "def call_llm_safe(prompt: str, max_tokens=512):\n",
        "    \"\"\"Call Gemini safely with retry if truncated.\"\"\"\n",
        "    global llm\n",
        "    result = llm.invoke(prompt)\n",
        "    text = extract_text(result)\n",
        "    finish_reason = \"\"\n",
        "    if hasattr(result, \"response_metadata\"):\n",
        "        finish_reason = result.response_metadata.get(\"finish_reason\", \"\")\n",
        "    # Retry if truncated or empty\n",
        "    if (not text or finish_reason == \"MAX_TOKENS\") and max_tokens < 1024:\n",
        "        print(\"âš ï¸ Gemini hit token limit, retrying with higher max_output_tokens...\")\n",
        "        time.sleep(0.5)\n",
        "        llm = make_llm(1024)\n",
        "        result = llm.invoke(prompt)\n",
        "        text = extract_text(result)\n",
        "    return text.strip()\n",
        "\n",
        "# ---- 5ï¸âƒ£ Node functions ----\n",
        "def analyze_mood(state: ChatState) -> ChatState:\n",
        "    user_text = state[\"messages\"][-1][\"content\"]\n",
        "    prompt = (\n",
        "        f\"Analyze this message and classify the user's mood as one word: \"\n",
        "        f\"happy, sad, angry, or neutral.\\n\\nMessage: {user_text}\"\n",
        "    )\n",
        "    mood_text = call_llm_safe(prompt)\n",
        "    mood = \"neutral\"\n",
        "    for m in [\"happy\", \"sad\", \"angry\", \"neutral\"]:\n",
        "        if m in mood_text.lower():\n",
        "            mood = m\n",
        "            break\n",
        "    return {**state, \"mood\": mood}\n",
        "\n",
        "def respond_based_on_mood(state: ChatState) -> ChatState:\n",
        "    user_text = state[\"messages\"][-1][\"content\"]\n",
        "    mood = state.get(\"mood\", \"neutral\")\n",
        "    prompt = (\n",
        "        f\"You are an empathetic assistant. The user feels {mood}. \"\n",
        "        f\"Respond naturally in 1â€“2 sentences to: '{user_text}'.\"\n",
        "    )\n",
        "    reply = call_llm_safe(prompt)\n",
        "    return {**state, \"reply\": reply or \"[no reply generated]\"}\n",
        "\n",
        "# ---- 6ï¸âƒ£ Build + compile ----\n",
        "graph = StateGraph(ChatState)\n",
        "graph.add_node(\"analyze_mood\", analyze_mood)\n",
        "graph.add_node(\"respond_based_on_mood\", respond_based_on_mood)\n",
        "graph.set_entry_point(\"analyze_mood\")\n",
        "graph.add_edge(\"analyze_mood\", \"respond_based_on_mood\")\n",
        "graph.add_edge(\"respond_based_on_mood\", END)\n",
        "\n",
        "interactive_app = graph.compile()\n",
        "\n",
        "# ===========================================\n",
        "# âœ… CELL 7 â€” Gemini Conversational LangGraph (Final Stable)\n",
        "# - Handles MAX_TOKENS cutoffs\n",
        "# - Robust LLM extraction\n",
        "# - Returns proper mood + reply each time\n",
        "# ===========================================\n",
        "\n",
        "from typing import TypedDict, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain.chat_models import init_chat_model\n",
        "import time\n",
        "\n",
        "# ---- 1ï¸âƒ£ State model ----\n",
        "class ChatState(TypedDict, total=False):\n",
        "    messages: List[dict]\n",
        "    mood: str\n",
        "    reply: str\n",
        "\n",
        "# ---- 2ï¸âƒ£ LLM setup ----\n",
        "def make_llm(max_tokens=512):\n",
        "    return init_chat_model(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        model_provider=\"google_genai\",\n",
        "        temperature=0.6,\n",
        "        max_output_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "llm = make_llm()\n",
        "\n",
        "# ---- 3ï¸âƒ£ Universal extractor ----\n",
        "def extract_text(result):\n",
        "    if isinstance(result, str):\n",
        "        return result.strip()\n",
        "    if hasattr(result, \"content\") and result.content:\n",
        "        return result.content.strip()\n",
        "    if hasattr(result, \"text\") and result.text:\n",
        "        return result.text.strip()\n",
        "    if hasattr(result, \"candidates\"):\n",
        "        try:\n",
        "            c = result.candidates[0]\n",
        "            if hasattr(c, \"content\") and getattr(c.content, \"parts\", None):\n",
        "                parts = c.content.parts\n",
        "                if parts and hasattr(parts[0], \"text\"):\n",
        "                    return parts[0].text.strip()\n",
        "        except Exception:\n",
        "            pass\n",
        "    if isinstance(result, dict):\n",
        "        if \"text\" in result:\n",
        "            return result[\"text\"].strip()\n",
        "    return \"\"\n",
        "\n",
        "# ---- 4ï¸âƒ£ Safe LLM caller with retry ----\n",
        "def call_llm_safe(prompt: str, max_tokens=512):\n",
        "    \"\"\"Call Gemini safely with retry if truncated.\"\"\"\n",
        "    global llm\n",
        "    result = llm.invoke(prompt)\n",
        "    text = extract_text(result)\n",
        "    finish_reason = \"\"\n",
        "    if hasattr(result, \"response_metadata\"):\n",
        "        finish_reason = result.response_metadata.get(\"finish_reason\", \"\")\n",
        "    # Retry if truncated or empty\n",
        "    if (not text or finish_reason == \"MAX_TOKENS\") and max_tokens < 1024:\n",
        "        print(\"âš ï¸ Gemini hit token limit, retrying with higher max_output_tokens...\")\n",
        "        time.sleep(0.5)\n",
        "        llm = make_llm(1024)\n",
        "        result = llm.invoke(prompt)\n",
        "        text = extract_text(result)\n",
        "    return text.strip()\n",
        "\n",
        "# ---- 5ï¸âƒ£ Node functions ----\n",
        "def analyze_mood(state: ChatState) -> ChatState:\n",
        "    user_text = state[\"messages\"][-1][\"content\"]\n",
        "    prompt = (\n",
        "        f\"Analyze this message and classify the user's mood as one word: \"\n",
        "        f\"happy, sad, angry, or neutral.\\n\\nMessage: {user_text}\"\n",
        "    )\n",
        "    mood_text = call_llm_safe(prompt)\n",
        "    mood = \"neutral\"\n",
        "    for m in [\"happy\", \"sad\", \"angry\", \"neutral\"]:\n",
        "        if m in mood_text.lower():\n",
        "            mood = m\n",
        "            break\n",
        "    return {**state, \"mood\": mood}\n",
        "\n",
        "def respond_based_on_mood(state: ChatState) -> ChatState:\n",
        "    user_text = state[\"messages\"][-1][\"content\"]\n",
        "    mood = state.get(\"mood\", \"neutral\")\n",
        "    prompt = (\n",
        "        f\"You are an empathetic assistant. The user feels {mood}. \"\n",
        "        f\"Respond naturally in 1â€“2 sentences to: '{user_text}'.\"\n",
        "    )\n",
        "    reply = call_llm_safe(prompt)\n",
        "    return {**state, \"reply\": reply or \"[no reply generated]\"}\n",
        "\n",
        "# ---- 6ï¸âƒ£ Build + compile ----\n",
        "graph = StateGraph(ChatState)\n",
        "graph.add_node(\"analyze_mood\", analyze_mood)\n",
        "graph.add_node(\"respond_based_on_mood\", respond_based_on_mood)\n",
        "graph.set_entry_point(\"analyze_mood\")\n",
        "graph.add_edge(\"analyze_mood\", \"respond_based_on_mood\")\n",
        "graph.add_edge(\"respond_based_on_mood\", END)\n",
        "\n",
        "interactive_app = graph.compile()\n",
        "\n",
        "# ---- 7ï¸âƒ£ Interactive loop ----\n",
        "print(\"ğŸ’¬ Interactive LangGraph + Gemini Chat (type 'exit' to quit)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "state: ChatState = {\"messages\": []}\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"ğŸ‘¤ You: \").strip()\n",
        "    if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "        print(\"ğŸ‘‹ Goodbye!\")\n",
        "        break\n",
        "\n",
        "    state.setdefault(\"messages\", []).append({\"role\": \"user\", \"content\": user_input})\n",
        "    result = interactive_app.invoke(state)\n",
        "    state.update(result)\n",
        "\n",
        "    reply = state.get(\"reply\", \"\").strip()\n",
        "    mood = state.get(\"mood\", \"N/A\")\n",
        "\n",
        "    # Append reply to memory for context\n",
        "    state[\"messages\"].append({\"role\": \"assistant\", \"content\": reply})\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ¤– LangGraph + Gemini Conversation Result\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"ğŸ§© Mood:\", mood)\n",
        "    print(\"ğŸ’¬ Reply:\", reply)\n",
        "    print(\"=\" * 60 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "Gz4w25ZZt9Bs",
        "outputId": "24f49f3a-f897-4a08-b887-993b7aa25e7e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¬ Interactive LangGraph + Gemini Chat (type 'exit' to quit)\n",
            "============================================================\n",
            "ğŸ‘¤ You: good day\n",
            "============================================================\n",
            "ğŸ¤– LangGraph + Gemini Conversation Result\n",
            "============================================================\n",
            "ğŸ§© Mood: neutral\n",
            "ğŸ’¬ Reply: Good day to you too! I hope you have a pleasant and productive day ahead.\n",
            "============================================================\n",
            "\n",
            "ğŸ‘¤ You: how was my day?\n",
            "âš ï¸ Gemini hit token limit, retrying with higher max_output_tokens...\n",
            "============================================================\n",
            "ğŸ¤– LangGraph + Gemini Conversation Result\n",
            "============================================================\n",
            "ğŸ§© Mood: neutral\n",
            "ğŸ’¬ Reply: I can't tell you exactly how your day was, but I truly hope it brought you some good moments. What was it like from your perspective?\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4226672124.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ‘¤ You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ‘‹ Goodbye!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5jXlCx997iwC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}